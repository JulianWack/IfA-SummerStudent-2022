{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1139c3",
   "metadata": {},
   "source": [
    "# Description\n",
    "Use Approximative Baysian Computation to fit $b1$ and $\\beta$ to model. Reduced $\\chi^2$ was chosen as a distance function. See `Power_in_different_densities.ipynb` for elementary fitting and background discussion. In ABC would need to compute inverse covariance matrix i.e. estimate for measurement error many times (for each candidate value of b1, beta). So speed up the computation, compute the covariance matrix not based on the linear model, but on the data. See https://arxiv.org/pdf/1509.04293.pdf equation 24. This removes the b1, beta dependence and only introduces an error of an error which can be ignored in light of the desired accuracy of this analysis. \n",
    "\n",
    "When trying to use `Pkmus[i] = r.power['mu']` as the i-th percentile data $P(k,\\mu)$ to estimate the covariance matrix, issues occur as a lot of elements are NaN (when no Fourier modes can contained in a k,mu bin to average the power over). Above alternative of using combination of multipoles weighted by Legendre polynomials works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d2f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d549cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import legendre, spherical_jn\n",
    "from scipy.integrate import simpson, quad\n",
    "from scipy.linalg import pinv\n",
    "import scipy.sparse as ss\n",
    "from scipy.stats import bayes_mvs\n",
    "\n",
    "import cat_power_algos as catpk\n",
    "import classylss\n",
    "import fitsio\n",
    "import zeus \n",
    "from nbodykit.lab import *\n",
    "from nbodykit import style, setup_logging\n",
    "#setup_logging()\n",
    "plt.style.use(style.notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0698097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwack/.conda/envs/nbodykit-env/lib/python3.8/site-packages/nbodykit/cosmology/cosmology.py:427: UserWarning: Class did not read input parameter(s): sigma_8, w0_fld, wa_fld\n",
      "  self.engine = ClassEngine(pars)\n",
      "/home/jwack/.conda/envs/nbodykit-env/lib/python3.8/site-packages/nbodykit/cosmology/cosmology.py:427: UserWarning: Class did not read input parameter(s): sigma_8\n",
      "  self.engine = ClassEngine(pars)\n"
     ]
    }
   ],
   "source": [
    "LOS = [0,0,1]\n",
    "redshift = 0\n",
    "BoxSize = 2000\n",
    "cosmo_paras = classylss.load_ini('/home/jwack/main/Planck18_LCDM.ini')\n",
    "cosmo = cosmology.cosmology.Cosmology.from_dict(cosmo_paras)\n",
    "Plin = cosmology.LinearPower(cosmo, redshift, transfer='EisensteinHu') # matter power spectrum \n",
    "\n",
    "dk = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d1e245",
   "metadata": {},
   "source": [
    "Load in density split data computed in `Power_in_different_densities.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857874e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptile_split = np.loadtxt('density_bins/percentile_edges.txt')\n",
    "\n",
    "ells = [0,2]\n",
    "n_ptile = len(ptile_split)-1 # number of bins = number of edges - 1\n",
    "Pkmus = np.empty(n_ptile, dtype='object') # contains P(k,mu) as 2D array of shape (#k, #mu) for each percentile\n",
    "Pk_ells = np.empty((n_ptile, len(ells)), dtype='object') # each column contains multipole of all percentiles\n",
    "\n",
    "for i in range(n_ptile):\n",
    "    r = FFTPower.load('density_bins/ptile_%d.json'%i)\n",
    "    poles = r.poles \n",
    "    mus = r.power.coords['mu']\n",
    "    \n",
    "    Pkmu_nl = np.zeros((len(poles['k']), len(mus)))\n",
    "    for j,ell in enumerate(ells):\n",
    "        Pk_ell = poles['power_%d' %ell].real\n",
    "        if ell == 0: \n",
    "            Pk_ell = Pk_ell - poles.attrs['shotnoise']\n",
    "            \n",
    "        Pk_ells[i][j] = Pk_ell\n",
    "        Pkmu_nl += np.outer(Pk_ell, legendre(ell)(mus))\n",
    "        \n",
    "    Pkmus[i] = Pkmu_nl\n",
    "\n",
    "k = poles['k']\n",
    "shotnoise = poles.attrs['shotnoise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aacac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_8(k, b1):\n",
    "    '''Computes sigma8 assuming linear matter power spectrum. \n",
    "    Follows https://physics.stackexchange.com/questions/521471/formula-to-compute-sigma8-for-correction-in-non-linear-regime and\n",
    "    https://nbodykit.readthedocs.io/en/latest/api/_autosummary/nbodykit.cosmology.power.linear.html?highlight=Spectra.sigma8#nbodykit.cosmology.power.linear.LinearPower.sigma_r'''\n",
    "    window = lambda x: 3*spherical_jn(1,x)/x\n",
    "    integrand = lambda k: window(8*k)**2 * b1**2 * Plin(k)\n",
    "    return 1/(2*np.pi**2) * quad(integrand, 1e-5, 10)\n",
    "\n",
    "\n",
    "def per_mode_cov(k, mus, Pkmu, l1, l2, shotnoise, dk):\n",
    "    '''Construct per mode covariance. See eq 15, 16 (for factor f) of Grieb et al. (2016).'''\n",
    "    V = BoxSize**3\n",
    "    V_k = 4/3*np.pi*((k+dk/2)**3 - (k-dk/2)**3)\n",
    "    f = 2*(2*np.pi)**4 / V_k**2 * k**2 * dk\n",
    "    L_l1, L_l2 = legendre(l1)(mus), legendre(l2)(mus)\n",
    "    integrand = (Pkmu + shotnoise)**2 * L_l1*L_l2\n",
    "    \n",
    "    return f*(2*l1+1)**2 * (2*l2+1)**2 / V * simpson(integrand, mus) # 1D array containing per mode cov for each k bin\n",
    "\n",
    "\n",
    "def gaussian_cov_mat_inv(k, mus, Pkmu, ells, shotnoise, dk):\n",
    "    '''See above markdown for explanation of structure of covariance matrix. Uses sparse matricies for fast inversion.\n",
    "    scipy.sparse.bmat allows to combine matricies by passing structure of larger matrix in terms of submatricies.'''\n",
    "    # initialize array accepting matricies as elements and fill with diagonal C_l1,l2 matricies\n",
    "    C = np.empty((len(ells), len(ells)), dtype='object')\n",
    "    for i,l1 in enumerate(ells):\n",
    "        for j,l2 in list(enumerate(ells))[i:]:\n",
    "            C[i][j] = ss.diags(per_mode_cov(k,mus,Pkmu,l1,l2,shotnoise,dk))\n",
    "            if j!=i:\n",
    "                C[j][i] = C[i][j]\n",
    "                \n",
    "    cov_mat = ss.bmat(C).tocsc() # convert to efficient scipy matrix format\n",
    "    \n",
    "    # deal with inverting signular matrix\n",
    "    try: \n",
    "        inv = ss.linalg.inv(cov_mat).toarray()\n",
    "    except RuntimeError:\n",
    "        inv = pinv(cov_mat.toarray())\n",
    "        \n",
    "    return inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eccb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(theta):\n",
    "    ''' The natural logarithm of the prior probability. Assume parameters independent such that log priors add.\n",
    "    Note that normalization is irrelevant for MCMC.'''\n",
    "    lp = 0.\n",
    "    b1, beta = theta\n",
    "    # choose uniform priors for both parameters. When parameter in range, set prior to 1 (log prior to 0) and when outside range\n",
    "    # set to 0 (log prior to -inf)\n",
    "    b1_min, b1_max = 0, 3\n",
    "    beta_min, beta_max = 0, 3\n",
    "    \n",
    "    lp_b1 = 0. if b1_min < b1 < b1_max else -np.inf\n",
    "    lp_beta = 0. if beta_min < beta < beta_max else -np.inf\n",
    "#     b1_mu, b1_sig = 1, 0.5\n",
    "#     lp_b1 = -0.5*((b1-b1_mu)/b1_sig)**2 #if beta > 0 else: -np.inf \n",
    "#     beta_mu, beta_sig = 1.1, 1\n",
    "#     lp_beta = -0.5*((beta-beta_mu)/beta_sig)**2 if beta > 0 else -np.inf \n",
    "\n",
    "    return lp_b1 + lp_beta\n",
    "\n",
    "\n",
    "def chi2(theta, data_multipoles, k, C_inv):\n",
    "    '''Return logarithm of likelihood (i.e. chi2) which is assumed to be normal.\n",
    "    Find chi^2 as explained in Fitting_b1.ipynb. data_multipoles must be an array of shape (len(ells), len(k)).\n",
    "    theta is parameter vector: [b1, beta].\n",
    "    Due to hard coding multipoles, only works for ells = [0,2]'''\n",
    "    b1, beta = theta\n",
    "    ells = [0,2]\n",
    "    # make model vector\n",
    "    model_multipoles = np.empty((len(ells), len(k)))\n",
    "    model_multipoles[0] = (1 + 2/3*beta + 1/5*beta**2) * b1**2 * Plin(k)\n",
    "    model_multipoles[1] = (4/3*beta + 4/7*beta**2) * b1**2 * Plin(k)\n",
    "        \n",
    "    D_M = (data_multipoles - model_multipoles).flatten()\n",
    "    \n",
    "    return D_M@(C_inv @ D_M)\n",
    "\n",
    "\n",
    "def logpost(theta, data_multipoles, k, C_inv):\n",
    "    '''Returns the logarithm of the posterior. By Bayes' theorem, this is just the sum of the log prior and log likelihood (up \n",
    "    to a irrelavant constant).\n",
    "    ''' \n",
    "    return logprior(theta) + chi2(theta, data_multipoles, k, C_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2bd29",
   "metadata": {},
   "source": [
    "Each walker explores the parameter space and produced a set of parameter values sampled from the posterior distribution. The number of walkers must be even and should be between 2 and 4 times the number of parameters (ndim). The starting point in parameter space of each walker is specified by `start`. From there `nsteps` along the Markov Chain are taken.\n",
    "\n",
    "To get access the sampled parameters, use `sampler.get_chain()`. Without any arguments this gives an array of shape `(nsteps, nwalkers, ndim)`. Set `flat=True` to get flat array. Can also specify that the first N samples of each walker (burn-in phase) should be discarded via `discard=N`. This is done before the flattening. Can also thin out chains.\n",
    "\n",
    "For more details consult Zeus documentation (https://zeus-mcmc.readthedocs.io/en/latest/index.html) and specifically this cookbook: https://zeus-mcmc.readthedocs.io/en/latest/notebooks/datafit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays full of NaN. When not enough candidates accepted, NaN remains and percentile will not be plotted\n",
    "b1_fits, beta_fits = np.full(n_ptile, np.nan), np.full(n_ptile, np.nan) # mean of accepted parameters\n",
    "b1_cred, beta_cred = np.full((n_ptile,2), np.nan), np.full((n_ptile,2), np.nan) # 3 sigma credibility interval\n",
    "reduced_chi2 = np.full(n_ptile, np.nan) # reduced chi at mean of accepted parameters \n",
    "\n",
    "ndim = 2\n",
    "nwalkers = 6 \n",
    "nsteps = 50\n",
    "\n",
    "start = 0.5 + np.random.random((nwalkers, ndim)) # initial positions: randomly choosen between 0.5 and 2.5 \n",
    "\n",
    "for i in range(n_ptile)[4:5]:\n",
    "    # change data format of Pk_ells to be compatible with chi2 minimization code \n",
    "    data_multipoles = np.zeros((len(ells), len(k)))\n",
    "    data_multipoles[0] = Pk_ells[i][0]\n",
    "    data_multipoles[1] = Pk_ells[i][1]\n",
    "    \n",
    "    C_inv = gaussian_cov_mat_inv(k, mus, Pkmus[i] , ells, shotnoise, dk)\n",
    "    sampler = zeus.EnsembleSampler(nwalkers, ndim, logpost, maxiter=1e5, args=[data_multipoles, k, C_inv]) # Initialise the sampler\n",
    "    sampler.run_mcmc(start, nsteps) # Run sampling\n",
    "    sampler.summary # Print summary diagnostics\n",
    "    \n",
    "    chain = sampler.get_chain(flat=True)#, discard=nsteps//2)\n",
    "    b1_stats, _, _ = bayes_mvs(chain[:,0], alpha=0.997) \n",
    "    beta_stats, _, _ = bayes_mvs(chain[:,1], alpha=0.997)\n",
    "    b1_fits[i], b1_cred[i][0], b1_cred[i][1] = b1_stats[0], b1_stats[1][0], b1_stats[1][1]\n",
    "    beta_fits[i], beta_cred[i][0], beta_cred[i][1] = beta_stats[0], beta_stats[1][0], beta_stats[1][1]\n",
    "    reduced_chi2[i] = chi2([b1_fits[i], beta_fits[i]], data_multipoles, k, C_inv) / (2*len(k)-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,1.5*ndim))\n",
    "for n in range(ndim):\n",
    "    plt.subplot2grid((ndim, 1), (n, 0))\n",
    "    plt.plot(sampler.get_chain()[:,:,n], alpha=0.5)\n",
    "#     fig.axes[0].set_ylabel('b1')\n",
    "#     fig.axes[1].set_ylabel('beta')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot marginal posterior distributions\n",
    "fig, axes = zeus.cornerplot(chain, labels=[r'$b1$', r'$\\beta$']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator\n",
    "fig, axs = plt.subplots(2, 2, figsize=(30,16), sharex=True)\n",
    "\n",
    "mids = np.arange(5,105,10)[1:] # percentile mid points\n",
    "\n",
    "axs[0][0].plot(mids, b1_fits[1:])\n",
    "axs[0][0].fill_between(mids, b1_cred[:,0][1:], b1_cred[:,1][1:], color='b', alpha=0.1, label=r'$3 \\: \\sigma$')\n",
    "axs[0][0].set_title(\"b1 fit\")\n",
    "axs[0][0].set_ylabel(r\"$b1$\")\n",
    "axs[0][0].legend()\n",
    "\n",
    "axs[0][1].plot(mids, beta_fits[1:])\n",
    "axs[0][1].fill_between(mids, beta_cred[:,0][1:], beta_cred[:,1][1:], color='b', alpha=0.1, label=r'$3 \\: \\sigma$')\n",
    "axs[0][1].set_title(r\"$\\beta$ fit\")\n",
    "axs[0][1].set_ylabel(r\"$\\beta$\")\n",
    "axs[0][1].legend()\n",
    "\n",
    "f0_true = cosmo.scale_independent_growth_rate(redshift)\n",
    "delta_f = (f0_true - (beta_fits*b1_fits)) / f0_true\n",
    "#delta_f_cred = (b1_cred.T*beta_fits+b1_fits*beta_cred.T) / f0_true\n",
    "axs[1][0].plot(mids, delta_f[1:])\n",
    "#axs[1][0].fill_between(mids, delta_f_cred[0][1:], delta_f_cred[1][1:], color='b', alpha=0.1, label=r'$3 \\: \\sigma$')\n",
    "axs[1][0].set_title(r\"difference to true growth rate\")\n",
    "axs[1][0].set_xlabel(\"density percentile\")\n",
    "axs[1][0].set_ylabel(r\"$(f^{true}_0 - \\beta*b1)/f^{true}_0$\")\n",
    "axs[1][0].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "axs[1][1].plot(mids, reduced_chi2[1:])\n",
    "axs[1][1].set_title(\"reduced $\\chi^2$\")\n",
    "axs[1][1].set_xlabel(\"density percentile\")\n",
    "axs[1][1].set_ylabel(r\"$\\chi^2 / dof$\")\n",
    "axs[1][1].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig('plots/Fitting_b1_beta_densitybins_MCMC.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188feeb",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "The ball park of the fitted parameter is similar to the one found in the fitting via minimizing chi2. The issue with the first percentile remains: The quadrupole moment is entirely negative such that a negative beta is fitted which gives a good chi2 but a wrong f0.\n",
    "\n",
    "The estimated f0 is better in this fit is better compared to the one from minimizing chi2. There still is a tendency to underestimate f0. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbody-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
